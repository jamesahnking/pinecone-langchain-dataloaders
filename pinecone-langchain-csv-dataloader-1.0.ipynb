{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```.csv``` loader and splitter\n",
    "\n",
    "1. Create ```/csvs``` directory then place all .docx files inside \n",
    "2. Run app and all ```.csv``` files within the directory will be loaded into your vector store\n",
    "3. Use helper function to delet db \n",
    "4. Use Chat functions to test\n",
    "5. Trace using LangServe\n",
    "---\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "├── pinecone-langchain-csv-dataloader.ipynb\n",
    "├── pdfs\n",
    "│   ├── client_file_01.csv\n",
    "│   ├── client_file_02.csv\n",
    "│   ├── client_file_03.csv\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ```check_and_load_csv_from_dir``` : checks directory for files,loads documents with the ```DirectoryLoader```, then splits the docs with langchains ```RecursiveCharacterTextSplitter```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet  docx2txt\n",
    "! pip install langchain\n",
    "! pip install tiktoken\n",
    "! pip install pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX - Check Path ald Load .pdf\n",
    "\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "def check_and_load_pdf_from_dir(directory):\n",
    "    # Ensure the directory path exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"Directory does not exist.\")\n",
    "        return False\n",
    "\n",
    "    # Check if directory is actually a directory\n",
    "    if not os.path.isdir(directory):\n",
    "        print(\"The specified path is not a directory.\")\n",
    "        return False\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "    print(f'.csv files within the /csvs directory: {all_files}')\n",
    "    \n",
    "    # Check if each file ends with .docx\n",
    "    for file in all_files:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            print(f\"Non-CSV file found: {file}\")\n",
    "            return False\n",
    " \n",
    " # load directory path\n",
    "    directory_path = directory \n",
    "    # add list comprehension for file os.path.join usage\n",
    "    loader = DirectoryLoader(directory_path,  glob=\"**/*.csv\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter( \n",
    "        chunk_size=1000,  # Maximum size of each chunk\n",
    "        chunk_overlap=100,  # Number of overlapping characters between chunks\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = check_and_load_pdf_from_dir('csvs')\n",
    "\n",
    "chunks\n",
    "# # Call Chunks \n",
    "# print(len(chunks))\n",
    "# print(chunks[0].page_content)\n",
    "# print(chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 99487\n",
      "Embedding Cost in USD:0.039795\n"
     ]
    }
   ],
   "source": [
    "# Split Data Text With Cost Calculation\n",
    "# How much it costs to embed\n",
    "def calculate_and_display_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enccoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    total_tokens = sum([len(enccoding.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD:{total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "calculate_and_display_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create and delete pinecone index functions\n",
    "- ```delete_index_with_same_name```: deletes pincone index of the same name \n",
    "- ```load_or_create_embeddings_index```: If the index already exists it will just load data. If the idex is brand new it will create an then load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import time\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\") or 'PINECONE_API_KEY'\n",
    ")\n",
    "\n",
    "def delete_index_with_same_name(index_name): \n",
    "    \n",
    "    # Delete index if any incdexes of the same name are present\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Deleting the {index_name} vector database')\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "\n",
    "def load_or_create_embeddings_index(index_name, chunks, namespace):\n",
    "    \n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings...', end='')\n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(), index_name=index_name, namespace=namespace)\n",
    "        \n",
    "        while not pc.describe_index(index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end = '')\n",
    "        pc.create_index(name=index_name, dimension=1536, metric='cosine',  spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-west-2'\n",
    "            ))\n",
    "        \n",
    "        while not pc.describe_index(index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "        # Add to vectorDB using LangChain \n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(), index_name=index_name, namespace=namespace)\n",
    "        print('Done')   \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ```calculate_and_display_embedding_cost```: calculate embedding cost using using tiktoken\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create your index and load all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index csv-rag-test-2 already exists. Loading embeddings...Done\n"
     ]
    }
   ],
   "source": [
    "index_name='csv-rag-test-2'\n",
    "chunks = chunks\n",
    "namespace = \"csv_documents\"\n",
    "\n",
    "vector_store = load_or_create_embeddings_index(index_name=index_name, chunks=chunks, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Delete your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete vector store \n",
    "index_name='pdf-rag-test-1'\n",
    "delete_index_with_same_name(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generate answer without context - simple answer\n",
    "- https://github.com/atef-ataya/LangChain-Tutorial/blob/master/Building%20QA%20application%20using%20OpenAI%2C%20Pinecone%2C%20and%20LangChain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A Chat Function \n",
    "def generate_answer_from_vector_store(vector_store, question):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    answer = chain.invoke(question)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Set up a chain for having a conversation based on retrieved documents.\n",
    "- https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain\n",
    "- https://github.com/atef-ataya/LangChain-Tutorial/blob/master/Building%20QA%20application%20using%20OpenAI%2C%20Pinecone%2C%20and%20LangChain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Conversation Logic and ChatHistory\n",
    "def conduct_conversation_with_context(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    \n",
    "    result = crc.invoke({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. This does something with LangSmith trace\n",
    "https://docs.smith.langchain.com/tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith Trace\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.environ['LANGCHAIN_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Chat with your database to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The employees who hold the position of Fire Suppression Captain in the provided data are MICHAEL CASTAGNOLA, MICHAEL DELANE, and GREG WYRSCH.\n",
      "[('What employee is the Fire Suppression Captain?', 'The employees who hold the position of Fire Suppression Captain in the provided data are MICHAEL CASTAGNOLA, MICHAEL DELANE, and GREG WYRSCH.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What employee is the Fire Suppression Captain?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeffrey Covitz makes a total of $160,383.74, according to the provided information for the year 2011.\n",
      "[('How much money does Jeffrey Covits make?', 'Jeffrey Covitz makes a total of $160,383.74, according to the provided information for the year 2011.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"How much money does Jeffrey Covits make?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate how much more money Jeffrey Covitz makes compared to Jonathan Huggins, we can look at their TotalPay:\n",
      "\n",
      "- Jeffrey Covitz's TotalPay: $160,383.74\n",
      "- Jonathan Huggins's TotalPay: $162,557.43\n",
      "\n",
      "Now we subtract Jonathan Huggins' TotalPay from Jeffrey Covitz's TotalPay:\n",
      "\n",
      "$160,383.74 - $162,557.43 = -$2,173.69\n",
      "\n",
      "Therefore, Jeffrey Covitz makes $2,173.69 less than Jonathan Huggins.\n",
      "[('How much more money does Jeffrey Covits make compared to Jonathan Huggins?', \"To calculate how much more money Jeffrey Covitz makes compared to Jonathan Huggins, we can look at their TotalPay:\\n\\n- Jeffrey Covitz's TotalPay: $160,383.74\\n- Jonathan Huggins's TotalPay: $162,557.43\\n\\nNow we subtract Jonathan Huggins' TotalPay from Jeffrey Covitz's TotalPay:\\n\\n$160,383.74 - $162,557.43 = -$2,173.69\\n\\nTherefore, Jeffrey Covitz makes $2,173.69 less than Jonathan Huggins.\")]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"How much more money does Jeffrey Covits make compared to Jonathan Huggins?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trasbotv0.1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
