{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```.pdf``` loader and splitter\n",
    "\n",
    "1. Create ```/pdfs``` directory then place all .docx files inside \n",
    "2. Run app and all ```.pdf``` files within the directory will be loaded into your vector store\n",
    "3. Use helper function to delet db \n",
    "4. Use Chat functions to test\n",
    "5. Trace using LangServe\n",
    "---\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "├── pinecone-langchain-pdf-dataloader.ipynb\n",
    "├── pdfs\n",
    "│   ├── client_file_01.pdf\n",
    "│   ├── client_file_02.pdf\n",
    "│   ├── client_file_03.pdf\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ```check_and_load_pdf_from_dir``` : checks directory for files,loads documents with the ```Docx2txtLoader```, then splits the docs with langchains ```RecursiveCharacterTextSplitter```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet  docx2txt\n",
    "! pip install langchain\n",
    "! pip install tiktoken\n",
    "! pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".pdf files within the /pdfs directory: ['CodeLlamaMeta.pdf', 'ArjanCodes-SDev-Guide.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Software Design Guide          \n",
      "A 7 step-guide to designing great software © ArjanCodes\n",
      "1  Before We Dive In Have you ever been stuck trying to find a way to write software that can solve a complex problem, but that doesn’t become a huge mess of spaghetti code? Do you often end up in a situation where you know what your software should eventually do, but you have no idea how or where to start? I’ve been there many times, just like you, and I’ve written this 7-step plan to help you create consistently great software designs. I’ve been developing software for as long as I can remember (but I have pretty bad memory, so there’s that\n"
     ]
    }
   ],
   "source": [
    "# DOCX - Check Path ald Load .pdf\n",
    "\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "\n",
    "def check_and_load_pdf_from_dir(directory):\n",
    "    # Ensure the directory path exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"Directory does not exist.\")\n",
    "        return False\n",
    "\n",
    "    # Check if directory is actually a directory\n",
    "    if not os.path.isdir(directory):\n",
    "        print(\"The specified path is not a directory.\")\n",
    "        return False\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "    print(f'.pdf files within the /pdfs directory: {all_files}')\n",
    "    \n",
    "    # Check if each file ends with .docx\n",
    "    for file in all_files:\n",
    "        if not file.endswith(\".pdf\"):\n",
    "            print(f\"Non-PDF file found: {file}\")\n",
    "            return False\n",
    " \n",
    " # load directory path\n",
    "    directory_path = directory \n",
    "    # add list comprehension for file os.path.join usage\n",
    "    pdf_files = [f for f in all_files if f.endswith(\".pdf\")]\n",
    "    # Create emptly list container\n",
    "    documents = []\n",
    "    # Loop through the directory, bundle and load docx files. \n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(directory_path, pdf_file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter( \n",
    "        chunk_size=1000,  # Maximum size of each chunk\n",
    "        chunk_overlap=100,  # Number of overlapping characters between chunks\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = check_and_load_pdf_from_dir('pdfs')\n",
    "\n",
    "# Call Chunks \n",
    "print(len(chunks))\n",
    "print(chunks[0].page_content)\n",
    "print(chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 2439\n",
      "Embedding Cost in USD:0.000976\n"
     ]
    }
   ],
   "source": [
    "# Split Data Text With Cost Calculation\n",
    "# How much it costs to embed\n",
    "def calculate_and_display_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enccoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    total_tokens = sum([len(enccoding.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD:{total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "calculate_and_display_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create and delete pinecone index functions\n",
    "- ```delete_index_with_same_name```: deletes pincone index of the same name \n",
    "- ```load_or_create_embeddings_index```: If the index already exists it will just load data. If the idex is brand new it will create an then load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesahnking/.pyenv/versions/3.11.8/envs/trasbotv0.1_env/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import time\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\") or 'PINECONE_API_KEY'\n",
    ")\n",
    "\n",
    "def delete_index_with_same_name(index_name): \n",
    "    \n",
    "    # Delete index if any incdexes of the same name are present\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Deleting the {index_name} vector database')\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "\n",
    "def load_or_create_embeddings_index(index_name, chunks, namespace):\n",
    "    \n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings...', end='')\n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(), index_name=index_name, namespace=namespace)\n",
    "        \n",
    "        while not pc.describe_index(index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end = '')\n",
    "        pc.create_index(name=index_name, dimension=1536, metric='cosine',  spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-west-2'\n",
    "            ))\n",
    "        \n",
    "        while not pc.describe_index(index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "        # Add to vectorDB using LangChain \n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(), index_name=index_name, namespace=namespace)\n",
    "        print('Done')   \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ```calculate_and_display_embedding_cost```: calculate embedding cost using using tiktoken\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create your index and load all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index pdf-rag-test-1 and embeddings ...Done\n"
     ]
    }
   ],
   "source": [
    "index_name='pdf-rag-test-1'\n",
    "chunks = chunks\n",
    "namespace = \"pdf_documents\"\n",
    "\n",
    "vector_store = load_or_create_embeddings_index(index_name=index_name, chunks=chunks, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Delete your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete vector store \n",
    "index_name='pdf-rag-test-1'\n",
    "delete_index_with_same_name(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generate answer without context - simple answer\n",
    "- https://github.com/atef-ataya/LangChain-Tutorial/blob/master/Building%20QA%20application%20using%20OpenAI%2C%20Pinecone%2C%20and%20LangChain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A Chat Function \n",
    "def generate_answer_from_vector_store(vector_store, question):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    answer = chain.invoke(question)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Set up a chain for having a conversation based on retrieved documents.\n",
    "- https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain\n",
    "- https://github.com/atef-ataya/LangChain-Tutorial/blob/master/Building%20QA%20application%20using%20OpenAI%2C%20Pinecone%2C%20and%20LangChain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Conversation Logic and ChatHistory\n",
    "def conduct_conversation_with_context(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    \n",
    "    result = crc.invoke({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. This does something with LangSmith trace\n",
    "https://docs.smith.langchain.com/tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith Trace\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.environ['LANGCHAIN_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Chat with your database to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"What is Llama?\"\n",
    "result, chat_history = conduct_conversation_with_context(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trasbotv0.1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
